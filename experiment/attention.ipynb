{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "True\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.cuda.device_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a character-level transformer model. The goal is to replicate a lecture to learn how to make a transformer architecture.\n",
    "\n",
    "This is a decoder-only architecture. There is no sequence-to-sequence functionality, no clasification of text, other NLP tasks... just text generation / pure language modeling.  \n",
    "\n",
    "Source:\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading & exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !mkdir ../data/tinyshakespeare -p\n",
    "# !mv input.txt ../data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe dataset size, first 1000 characters\n",
    "with open ('../data/tinyshakespeare/input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print (\"Length: \", len(text))\n",
    "print (text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocabulary size:  65\n"
     ]
    }
   ],
   "source": [
    "# Print sorted vocabulrary and vocabulary size\n",
    "vocab = sorted(set(text))\n",
    "print (\"Vocabulary: \", vocab)\n",
    "print (\"Vocabulary size: \", len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53, 1, 42, 47, 43, 1, 58, 46, 39, 52, 1, 58, 53, 1, 44, 39, 51, 47, 57, 46, 12, 0, 0, 13, 50, 50, 10, 0, 30, 43, 57, 53, 50, 60, 43, 42, 8, 1, 56, 43, 57, 53, 50, 60, 43, 42, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 18, 47, 56, 57, 58, 6, 1, 63, 53, 59, 1, 49, 52, 53, 61, 1, 15, 39, 47, 59, 57, 1, 25, 39, 56, 41, 47, 59, 57, 1, 47, 57, 1, 41, 46, 47, 43, 44, 1, 43, 52, 43, 51, 63, 1, 58, 53, 1, 58, 46, 43, 1, 54, 43, 53, 54, 50, 43, 8, 0, 0, 13, 50, 50, 10, 0, 35, 43, 1, 49, 52, 53, 61, 5, 58, 6, 1, 61, 43, 1, 49, 52, 53, 61, 5, 58, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 24, 43, 58, 1, 59, 57, 1, 49, 47, 50, 50, 1, 46, 47, 51, 6, 1, 39, 52, 42, 1, 61, 43, 5, 50, 50, 1, 46, 39, 60, 43, 1, 41, 53, 56, 52, 1, 39, 58, 1, 53, 59, 56, 1, 53, 61, 52, 1, 54, 56, 47, 41, 43, 8, 0, 21, 57, 5, 58, 1, 39, 1, 60, 43, 56, 42, 47, 41, 58, 12, 0, 0, 13, 50, 50, 10, 0, 26, 53, 1, 51, 53, 56, 43, 1, 58, 39, 50, 49, 47, 52, 45, 1, 53, 52, 5, 58, 11, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 1, 42, 53, 52, 43, 10, 1, 39, 61, 39, 63, 6, 1, 39, 61, 39, 63, 2, 0, 0, 31, 43, 41, 53, 52, 42, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 27, 52, 43, 1, 61, 53, 56, 42, 6, 1, 45, 53, 53, 42, 1, 41, 47, 58, 47, 64, 43, 52, 57, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 35, 43, 1, 39, 56, 43, 1, 39, 41, 41, 53, 59, 52, 58, 43, 42, 1, 54, 53, 53, 56, 1, 41, 47, 58, 47, 64, 43, 52, 57, 6, 1, 58, 46, 43, 1, 54, 39, 58, 56, 47, 41, 47, 39, 52, 57, 1, 45, 53, 53, 42, 8, 0, 35, 46, 39, 58, 1, 39, 59, 58, 46, 53, 56, 47, 58, 63, 1, 57, 59, 56, 44, 43, 47, 58, 57, 1, 53, 52, 1, 61, 53, 59, 50, 42, 1, 56, 43, 50, 47, 43, 60, 43, 1, 59, 57, 10, 1, 47, 44, 1, 58, 46, 43, 63, 0, 61, 53, 59, 50, 42, 1, 63, 47, 43, 50, 42, 1, 59, 57, 1, 40, 59, 58, 1, 58, 46, 43, 1, 57, 59, 54, 43, 56, 44, 50, 59, 47, 58, 63, 6, 1, 61, 46, 47, 50, 43, 1, 47, 58, 1, 61, 43, 56, 43, 0, 61, 46, 53, 50, 43, 57, 53, 51, 43, 6, 1, 61, 43, 1, 51, 47, 45, 46, 58, 1, 45, 59, 43, 57, 57, 1, 58, 46, 43, 63, 1, 56, 43, 50, 47, 43, 60, 43, 42, 1, 59, 57, 1, 46, 59, 51, 39, 52, 43, 50, 63, 11, 0, 40, 59, 58, 1, 58, 46, 43, 63, 1, 58, 46, 47, 52, 49, 1, 61, 43, 1, 39, 56, 43, 1, 58, 53, 53, 1, 42, 43, 39, 56, 10, 1, 58, 46, 43, 1, 50, 43, 39, 52, 52, 43, 57, 57, 1, 58, 46, 39, 58, 0, 39, 44, 44, 50, 47, 41, 58, 57, 1, 59, 57, 6, 1, 58, 46, 43, 1, 53, 40, 48, 43, 41, 58, 1, 53, 44, 1, 53, 59, 56, 1, 51, 47, 57, 43, 56, 63, 6, 1, 47, 57, 1, 39, 57, 1, 39, 52, 0, 47, 52, 60, 43, 52, 58, 53, 56, 63, 1, 58, 53, 1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47, 57, 43, 1, 58, 46, 43, 47, 56, 1, 39, 40, 59, 52, 42, 39, 52, 41, 43, 11, 1, 53, 59, 56, 0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43, 1, 47, 57, 1, 39, 1, 45, 39, 47, 52, 1, 58, 53, 1, 58, 46, 43, 51, 1, 24, 43, 58, 1, 59, 57, 1, 56, 43, 60, 43, 52, 45, 43, 1, 58, 46, 47, 57, 1, 61, 47, 58, 46, 0, 53, 59, 56, 1, 54, 47, 49, 43, 57, 6, 1, 43, 56, 43, 1, 61, 43, 1, 40, 43, 41, 53, 51, 43, 1, 56, 39, 49, 43, 57, 10, 1, 44, 53, 56, 1, 58, 46, 43, 1, 45, 53, 42, 57, 1, 49, 52, 53, 61, 1, 21, 0, 57, 54, 43, 39, 49, 1, 58, 46, 47, 57, 1, 47, 52, 1, 46, 59, 52, 45, 43, 56, 1, 44, 53, 56, 1, 40, 56, 43, 39, 42, 6, 1, 52, 53, 58, 1, 47, 52, 1, 58, 46, 47, 56, 57, 58, 1, 44, 53, 56, 1, 56, 43, 60, 43, 52, 45, 43, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer to encode and decode text by mapping vocabulary to integers\n",
    "stoi = {c: i for i, c in enumerate(vocab)}\n",
    "itos = {i: c for i, c in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[c] for c in l])\n",
    "\n",
    "print(encode(text[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and validation\n",
    "cutoff = int(0.9 * len(text))\n",
    "train_text = text[:cutoff]\n",
    "val_text = text[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2354886/560543220.py:1: UserWarning: Failed to initialize NumPy: numpy.core.multiarray failed to import (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:77.)\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data = data[: cutoff]\n",
    "val_data = data[cutoff:]\n",
    "(len(train_data), len(val_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading examples, batches, and creating an oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "For value 18 the target is 47\n",
      "For value 47 the target is 56\n",
      "For value 56 the target is 57\n",
      "For value 57 the target is 58\n",
      "For value 58 the target is 1\n",
      "For value 1 the target is 15\n",
      "For value 15 the target is 47\n",
      "For value 47 the target is 58\n"
     ]
    }
   ],
   "source": [
    "# Show the first input sequence and the expected predictions\n",
    "block_size = 8\n",
    "example = encode(train_text[:block_size + 1])\n",
    "example = torch.tensor(example, dtype=torch.long)\n",
    "print(example)\n",
    "\n",
    "for idx in range(block_size):\n",
    "    print(f'For value {example[idx]} the target is {example[idx+1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is [24] the target is 43\n",
      "When the input is [24, 43] the target is 58\n",
      "When the input is [24, 43, 58] the target is 5\n",
      "When the input is [24, 43, 58, 5] the target is 57\n",
      "When the input is [24, 43, 58, 5, 57] the target is 1\n",
      "When the input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "When the input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "When the input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "When the input is [44] the target is 53\n",
      "When the input is [44, 53] the target is 56\n",
      "When the input is [44, 53, 56] the target is 1\n",
      "When the input is [44, 53, 56, 1] the target is 58\n",
      "When the input is [44, 53, 56, 1, 58] the target is 46\n",
      "When the input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "When the input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "When the input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "When the input is [52] the target is 58\n",
      "When the input is [52, 58] the target is 1\n",
      "When the input is [52, 58, 1] the target is 58\n",
      "When the input is [52, 58, 1, 58] the target is 46\n",
      "When the input is [52, 58, 1, 58, 46] the target is 39\n",
      "When the input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "When the input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "When the input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "When the input is [25] the target is 17\n",
      "When the input is [25, 17] the target is 27\n",
      "When the input is [25, 17, 27] the target is 10\n",
      "When the input is [25, 17, 27, 10] the target is 0\n",
      "When the input is [25, 17, 27, 10, 0] the target is 21\n",
      "When the input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "When the input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "When the input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(is_train: bool = True):\n",
    "    data = train_data if is_train else val_data\n",
    "    ix = torch.randint(0, len(data)-block_size, (batch_size,)) # should this be len(data) - 1? we need to account for yb\n",
    "    xb = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    yb = torch.stack([data[i+1: i+1+block_size] for i in ix])\n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch()\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        print(f\"When the input is {xb[b][:t+1].tolist()} the target is {yb[b][t]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, y= None):\n",
    "        if y is None:\n",
    "            loss = None\n",
    "            pred = self.embed(x)\n",
    "\n",
    "        else:\n",
    "            pred = self.embed(x)\n",
    "            loss = F.cross_entropy(pred.view(-1, self.vocab_size), y.view(-1)) # must convert shapes from (B, T, C) to (B*T, C) channels must be second!\n",
    "        return pred, loss\n",
    "\n",
    "    def generate(self, x, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits = self(x)[0] \n",
    "            last = logits[:, -1, :] # get prediction for latest token only\n",
    "            probs = F.softmax(last, dim=-1)  # normalize logits to prob distr\n",
    "            val = torch.multinomial(probs, num_samples=1) #sample from distr\n",
    "            x = torch.cat([x,val], dim=1) # add sampled char to input\n",
    "        return x\n",
    "\n",
    "\n",
    "m = BigramModel(len(vocab))\n",
    "print(m(xb, yb)[0].shape, m(xb,yb)[1])\n",
    "\n",
    "test = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(test, 100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6924, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6463, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4171, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3734, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2321, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8495, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8547, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7979, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5864, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5940, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5129, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3269, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3191, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2556, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2865, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0176, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0480, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0441, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0933, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0888, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8663, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9813, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8465, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7865, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7285, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7645, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7316, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7336, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6268, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6779, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8629, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6697, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6403, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5446, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5532, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5864, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5846, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5932, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5326, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6924, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5711, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4539, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5789, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6147, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10000):\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A:\n",
      "GLeco wh hangotLO:ws, oraingor, s ve!\n",
      "A:\n",
      "\n",
      "\n",
      "Theleseeserer hee an beeOFonoreme ain cketoty dedo lo'lllI at ta d:\n",
      "ELIS me turf lal y his d w pe atho oraingre n y t\n",
      "Enganoreralo anicererupa anse trcor\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(test, 200)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.9964e-01, 8.4944e-01],\n",
      "         [7.7692e-01, 6.9311e-01],\n",
      "         [7.9659e-01, 7.0895e-01],\n",
      "         [9.8861e-01, 5.0518e-02],\n",
      "         [4.8716e-01, 5.3531e-01],\n",
      "         [1.7477e-01, 8.2057e-01],\n",
      "         [1.8916e-02, 4.4383e-01],\n",
      "         [6.2715e-01, 3.0160e-01]],\n",
      "\n",
      "        [[3.7760e-01, 9.7373e-01],\n",
      "         [3.9331e-02, 9.7179e-01],\n",
      "         [7.6383e-01, 8.0790e-01],\n",
      "         [7.0879e-02, 6.1775e-01],\n",
      "         [6.1360e-01, 6.8784e-04],\n",
      "         [9.5227e-01, 1.4038e-01],\n",
      "         [6.0251e-02, 8.2354e-01],\n",
      "         [5.2606e-01, 7.3723e-01]],\n",
      "\n",
      "        [[6.7681e-02, 9.0671e-02],\n",
      "         [5.9465e-01, 3.9542e-01],\n",
      "         [4.4005e-01, 6.1657e-01],\n",
      "         [5.1777e-01, 2.7781e-01],\n",
      "         [6.3437e-01, 4.8064e-01],\n",
      "         [8.9553e-01, 2.4832e-01],\n",
      "         [8.4402e-01, 5.6527e-01],\n",
      "         [9.8648e-01, 8.4320e-01]],\n",
      "\n",
      "        [[6.7831e-01, 2.5030e-01],\n",
      "         [8.8675e-01, 7.9661e-01],\n",
      "         [4.5590e-01, 6.3645e-02],\n",
      "         [2.6560e-01, 9.9764e-02],\n",
      "         [1.8401e-01, 1.7199e-01],\n",
      "         [1.5863e-01, 8.9662e-01],\n",
      "         [2.3094e-02, 9.9976e-01],\n",
      "         [1.4817e-01, 5.7109e-01]]])\n",
      "tensor([[[0.3996, 0.8494],\n",
      "         [0.5883, 0.7713],\n",
      "         [0.6577, 0.7505],\n",
      "         [0.7404, 0.5755],\n",
      "         [0.6898, 0.5675],\n",
      "         [0.6040, 0.6096],\n",
      "         [0.5204, 0.5860],\n",
      "         [0.5337, 0.5504]],\n",
      "\n",
      "        [[0.3776, 0.9737],\n",
      "         [0.2085, 0.9728],\n",
      "         [0.3936, 0.9178],\n",
      "         [0.3129, 0.8428],\n",
      "         [0.3730, 0.6744],\n",
      "         [0.4696, 0.5854],\n",
      "         [0.4111, 0.6194],\n",
      "         [0.4255, 0.6341]],\n",
      "\n",
      "        [[0.0677, 0.0907],\n",
      "         [0.3312, 0.2430],\n",
      "         [0.3675, 0.3676],\n",
      "         [0.4050, 0.3451],\n",
      "         [0.4509, 0.3722],\n",
      "         [0.5250, 0.3516],\n",
      "         [0.5706, 0.3821],\n",
      "         [0.6226, 0.4397]],\n",
      "\n",
      "        [[0.6783, 0.2503],\n",
      "         [0.7825, 0.5235],\n",
      "         [0.6737, 0.3702],\n",
      "         [0.5716, 0.3026],\n",
      "         [0.4941, 0.2765],\n",
      "         [0.4382, 0.3798],\n",
      "         [0.3789, 0.4684],\n",
      "         [0.3501, 0.4812]]])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.rand((4, 8, 2))\n",
    "xbow = torch.rand(4,8,2)\n",
    "print(sample)\n",
    "\n",
    "for i in range(sample.shape[0]): # iterate on batch\n",
    "    for j in range(sample.shape[1]): #iterate to tth token\n",
    "        xprev = sample[i][0:j+1]\n",
    "        xbow[i][j] = xprev.mean(0)\n",
    "\n",
    "print(xbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 5],\n",
      "        [0, 6],\n",
      "        [1, 2]])\n",
      "tensor([[ 1,  5],\n",
      "        [ 1, 11],\n",
      "        [ 2, 13]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 10, (3,2))\n",
    "print(a)\n",
    "print(mask @ a)\n",
    "# We get a cumulative sum. note how pytorch is height by width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((8,8), dtype=torch.long))  \n",
    "mask = mask / mask.sum(dim=1, keepdim=True)\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3996, 0.8494],\n",
       "         [0.5883, 0.7713],\n",
       "         [0.6577, 0.7505],\n",
       "         [0.7404, 0.5755],\n",
       "         [0.6898, 0.5675],\n",
       "         [0.6040, 0.6096],\n",
       "         [0.5204, 0.5860],\n",
       "         [0.5337, 0.5504]],\n",
       "\n",
       "        [[0.3776, 0.9737],\n",
       "         [0.2085, 0.9728],\n",
       "         [0.3936, 0.9178],\n",
       "         [0.3129, 0.8428],\n",
       "         [0.3730, 0.6744],\n",
       "         [0.4696, 0.5854],\n",
       "         [0.4111, 0.6194],\n",
       "         [0.4255, 0.6341]],\n",
       "\n",
       "        [[0.0677, 0.0907],\n",
       "         [0.3312, 0.2430],\n",
       "         [0.3675, 0.3676],\n",
       "         [0.4050, 0.3451],\n",
       "         [0.4509, 0.3722],\n",
       "         [0.5250, 0.3516],\n",
       "         [0.5706, 0.3821],\n",
       "         [0.6226, 0.4397]],\n",
       "\n",
       "        [[0.6783, 0.2503],\n",
       "         [0.7825, 0.5235],\n",
       "         [0.6737, 0.3702],\n",
       "         [0.5716, 0.3026],\n",
       "         [0.4941, 0.2765],\n",
       "         [0.4382, 0.3798],\n",
       "         [0.3789, 0.4684],\n",
       "         [0.3501, 0.4812]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask @ sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = torch.tril(torch.ones((8,8), dtype=torch.float))\n",
    "val[val==0] = -torch.inf\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(val, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4769,  1.1327,  0.7731,  ..., -0.2679,  0.6497,  0.0312],\n",
       "         [ 0.0106, -0.2966, -0.1085,  ...,  0.4612, -0.0433,  0.5922],\n",
       "         [-0.1206,  0.1316,  0.0050,  ...,  0.0691, -0.1379,  0.4689],\n",
       "         ...,\n",
       "         [-0.1779,  0.3911, -0.0929,  ..., -0.1690, -0.1640,  0.1042],\n",
       "         [-0.3149,  0.0428, -0.4238,  ...,  0.1710, -0.1707,  0.0250],\n",
       "         [-0.2148,  0.3741, -0.0986,  ...,  0.0213, -0.0842,  0.2443]],\n",
       "\n",
       "        [[-1.8004,  0.0076, -0.0238,  ..., -0.7904, -0.1716,  0.1303],\n",
       "         [-0.9701,  0.0128, -0.1903,  ..., -0.3677, -0.4117,  0.0375],\n",
       "         [-0.1394,  0.4624, -0.1384,  ...,  0.0072, -0.0533,  0.0413],\n",
       "         ...,\n",
       "         [ 0.1976, -0.1131, -0.1375,  ...,  0.1324, -0.5433, -0.0589],\n",
       "         [ 0.0924, -0.1698, -0.1323,  ..., -0.0724, -0.2501, -0.0958],\n",
       "         [ 0.1734, -0.1607, -0.1650,  ...,  0.0443, -0.4195, -0.1125]],\n",
       "\n",
       "        [[ 0.6144,  0.7227, -0.4373,  ..., -0.4756, -0.1982,  0.6854],\n",
       "         [ 0.2863,  0.2347,  0.0836,  ..., -0.1338, -0.2028,  0.2630],\n",
       "         [ 0.0933,  0.4160,  0.1803,  ..., -0.1767, -0.3376,  0.1379],\n",
       "         ...,\n",
       "         [ 0.1807,  0.4156, -0.2406,  ..., -0.2882, -0.1116,  0.2649],\n",
       "         [ 0.1443,  0.2079, -0.0801,  ..., -0.1497, -0.0687,  0.0226],\n",
       "         [ 0.1071,  0.1618, -0.1348,  ..., -0.2870, -0.0910,  0.0515]],\n",
       "\n",
       "        [[ 0.5706,  0.4057, -0.3190,  ..., -0.3261, -0.3139,  0.0979],\n",
       "         [-0.1778,  0.0720, -0.4193,  ..., -0.2286, -0.0983, -0.1659],\n",
       "         [ 0.5539,  0.3593, -0.0495,  ..., -0.2002,  0.0110, -0.0052],\n",
       "         ...,\n",
       "         [ 0.1715,  0.2231, -0.2074,  ..., -0.0540, -0.1374, -0.0831],\n",
       "         [ 0.1918,  0.2301, -0.0908,  ..., -0.0500, -0.0340,  0.0091],\n",
       "         [ 0.1455,  0.1477, -0.0812,  ..., -0.0635, -0.0677,  0.0255]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "torch.manual_seed(1337)\n",
    "n_embed = 65 # same as vocab\n",
    "xb = torch.randn((4, 8, n_embed))\n",
    "\n",
    "B, T, C = xb.shape\n",
    "head_size = 32\n",
    "\n",
    "# Self attention values (B, T, H) (smaller headsize, as attention calc has O(n^2))\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "q = query(xb)\n",
    "k = key(xb)\n",
    "v = value(xb)\n",
    "\n",
    "# implement attention formula softmax(Q*K/sqrt(H)) * V\n",
    "affinity = q @ k.transpose(-2, -1) # (B,T,H) * (B,H,T) -> (B,T,T) we see this is context squared in dimensions, not embeddings squared...\n",
    "affinity = torch.masked_fill(affinity, torch.tril(torch.ones(T,T)) == 0, -torch.inf)/ math.sqrt(head_size)\n",
    "wei= torch.softmax(affinity, dim=-1)  # B T T\n",
    "out = wei @ v\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4166, 0.5834, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3572, 0.4355, 0.2072, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3124, 0.2359, 0.1973, 0.2544, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1460, 0.2074, 0.1991, 0.2169, 0.2305, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2731, 0.1570, 0.2305, 0.1111, 0.1061, 0.1222, 0.0000, 0.0000],\n",
       "         [0.1197, 0.0957, 0.0872, 0.0999, 0.2224, 0.3185, 0.0567, 0.0000],\n",
       "         [0.2045, 0.0996, 0.1021, 0.1003, 0.1049, 0.1006, 0.1552, 0.1328]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6872, 0.3128, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2581, 0.2713, 0.4705, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2696, 0.2137, 0.1674, 0.3493, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2369, 0.1408, 0.1960, 0.2376, 0.1887, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1553, 0.3391, 0.0995, 0.1282, 0.1211, 0.1568, 0.0000, 0.0000],\n",
       "         [0.1288, 0.1180, 0.0693, 0.1153, 0.0818, 0.1982, 0.2887, 0.0000],\n",
       "         [0.1158, 0.2204, 0.0817, 0.1625, 0.1017, 0.1332, 0.1075, 0.0771]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4817, 0.5183, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2402, 0.4111, 0.3487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1776, 0.2175, 0.1894, 0.4155, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1331, 0.2324, 0.1203, 0.3200, 0.1942, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2861, 0.1010, 0.1849, 0.2114, 0.1094, 0.1071, 0.0000, 0.0000],\n",
       "         [0.1226, 0.1761, 0.1224, 0.2063, 0.1303, 0.0963, 0.1460, 0.0000],\n",
       "         [0.0651, 0.1184, 0.0907, 0.2251, 0.1380, 0.1468, 0.0927, 0.1232]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4359, 0.5641, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4072, 0.2020, 0.3908, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2309, 0.1357, 0.4051, 0.2283, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1024, 0.1886, 0.3270, 0.1789, 0.2031, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1989, 0.0935, 0.1163, 0.2634, 0.2371, 0.0908, 0.0000, 0.0000],\n",
       "         [0.1784, 0.1553, 0.1764, 0.1499, 0.1155, 0.1188, 0.1056, 0.0000],\n",
       "         [0.1884, 0.1395, 0.1336, 0.1722, 0.0927, 0.0844, 0.0970, 0.0920]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-nlp-viKy0CXJ-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7b9f912ab50c079aa39aa960a96a43b438d67449b46838a162e9689693b098f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
